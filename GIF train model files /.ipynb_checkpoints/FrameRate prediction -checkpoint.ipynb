{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "involved-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "from tensorflow.keras import layers \n",
    "import bert \n",
    "import re \n",
    "# re — Regular expression operations\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd                     \n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras import losses\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efficient-greensboro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125782, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/tgif-v1.0.tsv\", sep='\\t')\n",
    "data.isnull().values.any()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "composed-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_links = list(data.y.values)\n",
    "raw_tweets = list(data.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "instrumental-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "challenging-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-cliff",
   "metadata": {},
   "source": [
    "# Tweet Pre-Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-catch",
   "metadata": {},
   "source": [
    "## Remove Special Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "severe-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for function for removing html tags \n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "roman-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for function for remove any punctuations and special characters\n",
    "def preprocess_text(raw_tweaet):\n",
    "    # Removing html tags\n",
    "    tweet = remove_tags(raw_tweaet)\n",
    "    # Removing html tags\n",
    "    tweet = re.sub('[^a-zA-Z]', '', tweet)\n",
    "    # Removing html tags\n",
    "    tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', tweet)\n",
    "    # Removing multiple spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scheduled-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the preprocess_text function to clean tweets list \n",
    "tweets = [] \n",
    "for tweet in raw_tweets[:50]:\n",
    "    tweets.append(preprocess_text(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-panama",
   "metadata": {},
   "source": [
    "## Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "editorial-protest",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8d0b246c9517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBertTokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n\u001b[0;32m----> 4\u001b[0;31m                             trainable=False)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvocabulary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# .numpy(): converts a tensor object into an numpy.ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     return resolver.atomic_download(handle, download, module_dir,\n\u001b[0;32m---> 68\u001b[0;31m                                     self._lock_file_timeout_sec())\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_lock_file_timeout_sec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36matomic_download\u001b[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading TF-Hub Module '%s'.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mdownload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;31m# Write module descriptor to capture information about which module was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;31m# downloaded by whom and when. The file stored at the same level as a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/compressed_module_resolver.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(handle, tmp_dir)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       return resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[0;32m---> 65\u001b[0;31m           response, tmp_dir)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     return resolver.atomic_download(handle, download, module_dir,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36mdownload_and_uncompress\u001b[0;34m(self, fileobj, dst_path)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       file_utils.extract_tarfile_to_destination(\n\u001b[0;32m--> 190\u001b[0;31m           fileobj, dst_path, log_function=self._log_progress)\n\u001b[0m\u001b[1;32m    191\u001b[0m       total_size_str = tf_utils.bytes_to_readable_str(\n\u001b[1;32m    192\u001b[0m           self._total_bytes_downloaded, True)\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/file_utils.py\u001b[0m in \u001b[0;36mextract_tarfile_to_destination\u001b[0;34m(fileobj, dst_path, log_function)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mextract_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_target_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_target_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow_hub/file_utils.py\u001b[0m in \u001b[0;36mextract_file\u001b[0;34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid compressed data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbuf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a tokenizer \n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "# .numpy(): converts a tensor object into an numpy.ndarray\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for function for convert tweet to ids \n",
    "def tokenize_tweets(text_tweets):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tokenize_tweets on tweets \n",
    "tokenized_tweets = [tokenize_tweets(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweets = pd.read_csv(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/tokenized_tweets.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-vector",
   "metadata": {},
   "source": [
    "# GIF Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Requests is an elegant and simple HTTP library for Python\n",
    "import os \n",
    "# os — Miscellaneous operating system interfaces¶\n",
    "os.chdir('/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/gifs')\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gif_downloader(image_urls, status=[], filenames = []):\n",
    "    \n",
    "    for index, img in enumerate(image_urls):\n",
    "        # We can split the file based upon / and extract the last split within the python list below:\n",
    "        file_name = img.split('/')[-1]\n",
    "        #print(\"fThis is the file name: {file_name}\")\n",
    "        filenames.append(file_name) \n",
    "        # Now let's send a request to the image URL:\n",
    "        r = requests.get(img, stream=True)\n",
    "        # We can check that the status code is 200 before doing anything else:\n",
    "        if r.status_code == 200:\n",
    "            # This command below will allow us to write the data to a file as binary:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "            status.append(True)\n",
    "        else:\n",
    "            # We will write all of the images back to the broken_images list:\n",
    "            status.append(False)\n",
    "    return filenames, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_gif(image_path, sess):\n",
    "#     image = tf.io.read_file(image_file)\n",
    "#     image = tf.io.decode_gif(image)\n",
    "#     return sess.run(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_status = []\n",
    "filenames = []\n",
    "filenames, downlod_status = gif_downloader(gif_links[:50], status=download_status)\n",
    "# if any gif was not downloaded successfully \n",
    "print(any(status == False for status in download_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = pd.read_csv(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/filenames.csv\", header=None)\n",
    "filenames = filenames.iloc[1:]\n",
    "filenames = filenames.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fps(PIL_Image_object):\n",
    "    \"\"\" Returns the average framerate of a PIL Image object \"\"\"\n",
    "    PIL_Image_object.seek(0)\n",
    "    frames = duration = 0\n",
    "    while True:\n",
    "        try:\n",
    "            frames += 1\n",
    "            duration += PIL_Image_object.info['duration']\n",
    "            PIL_Image_object.seek(PIL_Image_object.tell() + 1)\n",
    "        except EOFError:\n",
    "            return frames / duration * 1000\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In GIF files, each frame has its own duration. So there is no general fps for a GIF file. \n",
    "framerates = []\n",
    "for gif_file_path in filenames[:50]: \n",
    "#     cap=cv2.VideoCapture(\"gif_file_path\")\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    gif_obj = Image.open(gif_file_path)\n",
    "    framerates.append(get_avg_fps(gif_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.max(framerates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-portfolio",
   "metadata": {},
   "source": [
    "# Prerparing Data For Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenized_tweets\n",
    "y_train = framerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_len = [[tweet, y_train[i], len(tweet)]\n",
    "                 for i, tweet in enumerate(x_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by tweet length \n",
    "tweets_with_len.sort(key=lambda x: x[2])\n",
    "#remove the tweet length attribute from dataset \n",
    "sorted_tweets_labels = [(tweet_lab[0], tweet_lab[1]) for tweet_lab in tweets_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_tweets_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_tweets_labels, output_types=(tf.int32, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10 \n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = math.ceil(len(sorted_tweets_labels) / BATCH_SIZE)\n",
    "# TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "TEST_BATCHES = 1\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-turkish",
   "metadata": {},
   "source": [
    "# Creating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension.\n",
    "#allows the model to handle input of variable length, in the simplest way possible\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(VOCAB_LENGTH + 1, EMB_DIM),\n",
    "    layers.Dropout(0.2),    \n",
    "    layers.GlobalAveragePooling1D(), \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='linear') #set activation to linear for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(y=history.history['loss'],\n",
    "                    name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scattergl(y=history.history['val_loss'],\n",
    "                     name='Valid'))\n",
    "\n",
    "fig.update_layout(height=500, width=700,\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "\n",
    "# import joblib\n",
    "# model.save(\"/Users/zimingfang/Desktop/Animated GIFs/AwesomeGif/GIF models/framerate_prediction_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-expert",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
